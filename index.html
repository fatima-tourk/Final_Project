<!doctype html>
<html lang="en">
<head>
<title>Language Models are Unsupervised Multitask Learners</title>
<meta property="og:title" content="Language Models are Unsupervised Multitask Learners" />
<meta name="twitter:title" content="Language Models are Unsupervised Multitask Learners" />
<meta name="description" content="Examining a paper demonstrating the ability of large scale language models to complete zero shot task transfer." />
<meta property="og:description" content="Examining a paper demonstrating the ability of large scale language models to complete zero shot task transfer." />
<meta name="twitter:description" content="Examining a paper demonstrating the ability of large scale language models to complete zero shot task transfer." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Language Models are Unsupervised Multitask Learners</nobr>
 <nobr class="widenobr">for CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of Language Models are Unsupervised Multitask Learners</h2>
<p>Examining a paper demonstrating the ability of large scale language models (GPT-2) to complete zero shot task transfer, a foundational property of subsequent GPT-3.5 and GPT-4 models.</p>
</div>
</div>
<div class="row">
<div class="col">

  <div class="row">
    <div class="col">
  
      <!-- Literature Review Section -->
      <section>
        <h2>Literature Review</h2>
        <ul>
          <li><strong>Word Embeddings (2013):</strong> Mikolov et al. (2013) and Pennington et al. (2014) introduced Word2Vec and GloVe embeddings, revolutionizing word representation in continuous vector spaces for better generalization. </li>
          <li><strong>Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks (2014):</strong> Suskever et al. and Cho et al. (2014) advanced Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks, enhancing sequential dependency capture for tasks like language modeling and machine translation. </li>
          <li><strong>Attention Mechanism (2017):</strong> The introduction of attention mechanisms, as seen in the Transformer architecture by Vaswani et al, revolutionized sequence-to-sequence tasks. The self-attention mechanism allowed models to focus on different parts of the input sequence when making predictions, leading to improved performance.</li>
          <li><strong>The Goldilocks Principle: Reading Childrens Books with Explicit Memory Representations (2017): </strong> This paper explored the idea of utilizing explicit memory representations to improve language understanding. It contributed to the understanding of how memory mechanisms can enhance NLP tasks.</li>
          <li><strong>Transfer Learning in NLP (2018):</strong> Researchers started leveraging transfer learning in NLP, pre-training models on large datasets for general language understanding tasks and then fine-tuning them for specific tasks. OpenAI's GPT (Generative Pre-trained Transformer) was a notable example of this trend.</li>
          <li><strong>Exploring the Limits of Language Modeling (2018):</strong> This paper by. OpenAI contributed insights into the scaling of language models, showing that larger models trained on massive datasets could achieve better performance. The findings in this paper set the stage for the development of even more advanced models like GPT-2 and GPT-3.</li>
          <li><strong>BERT (Bidirectional Encoder Representations from Transformers) (2018):</strong> Google's BERT introduced bidirectional context representation. It popularized the use of large-scale pre-training on massive corpora, showcasing the effectiveness of unsupervised learning for language understanding.</li>
        </ul>
      </section>
  
      <!-- Biography Section -->
      <section>
        <h2>Biography</h2>

        <!-- Alec Radford -->
        <h3>Alec Radford</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/alec radford.jpeg" alt="Alec Radford Image" width="300" height="200">
          <li>The original paper on generative pre-training of a transformer-based language model was written by Alec Radford (GPT-1)</li>
          <li>Franklin W. Olin College of Engineering</li>
          <li>Head of research at indico data solutions</li>
          <li>ML developer/researcher at OpenAI</li>
          <li>104767 citations</li>
          <li><a href="https://twitter.com/AlecRad" target="_blank">Twitter</a></li>
          <li><a href="https://www.linkedin.com/in/alecradford/" target="_blank">LinkedIn</a></li>
        </ul>

        <!-- Jeffrey Wu -->
        <h3>Jeffrey Wu</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/jeff wu.jpg" alt="Jeffrey Wu Image" width="300" height="200">
          <li>MIT Undergraduate</li>
          <li>MIT Masters in Probabilistic Programming Languages</li>
          <li>2nd employee at startup Terminal.com, left after 4 years and Terminal was later acquired by Udacity</li>
          <li>2 years at Google working on deep learning for user personalization</li>
          <li>Now at OpenAI on the alignment team, working to ensure that AI is truthful and helpful</li>
          <li>40650 citations</li>
          <li><a href="https://www.wuthejeff.com/about/" target="_blank">Website</a></li>
        </ul>

        <!-- Rewon Child -->
        <h3>Rewon Child</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/rewon child.png" alt="Rewon Child Image" width="200" height="200">
          <li>2022 - Present: Founding team, Member of Technical Staff at Inflection</li>
          <li>2021 - 2022: Research at Google Brain/Search on large models.</li>
          <li>2018 - 2020: Algorithms team at OpenAI</li>
          <li>2016 - 2017: Speech team at Andrew Ng's lab at Baidu Research.</li>
          <li>2015 - 2016: Enlitic, a startup focused on applying deep learning to medical imaging.</li>
          <li>44589 citations</li>
          <li><a href="https://rewonc.github.io//" target="_blank">Website</a></li>
        </ul>

        <!-- David Luan -->
        <h3>David Luan</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/david-figs-v1B.png" alt="David Luan Image">
          <li>VP of engineering at OpenAI</li>
          <li>Most of my career has revolved around near- and long-term impacts of AI on society</li>
          <li>Certificate in CS from Worcester State from when I was 12, and a BS/BA in Applied Math and Political Science from Yale</li>
          <li>Former director of AI at Axon</li>
          <li>Founded and was CEO of Dextro</li>
          <li>11276 citations</li>
          <li><a href="http://www.davidluan.com/" target="_blank">Website</a></li>
        </ul>

        <!-- Dario Amodei -->
        <h3>Dario Amodei</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/dario amodei.jpeg" alt="Dario Amodei Image">
          <li>CEO and Co-founder of anthropic, AI safety and research company</li>
          <li>Quit OpenAI to build rival company in 2020</li>
          <li>Was VP of research, research director, and team lead for AI safety</li>
          <li>Senior Research scientist at google</li>
          <li>Research Scientist at Baidu Inc with Andrew Ng</li>
          <li>Undergraduate from Stanford in Physics</li>
          <li>PhD from Princeton in Biophysics</li>
          <li>44885 citations</li>
          <li><a href="https://www.linkedin.com/in/dario-amodei-3934934/" target="_blank">LinkedIn</a></li>
        </ul>

        <!-- Ilya Sutskever -->
        <h3>Ilya Sutskever</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/ilya sutskever.jpg" alt="Ilya Sutskever Image" width="300" height="200">
          <li>Co-founder and chief scientist at OpenAI</li>
          <li>Co-inventor, with Alex Krizhevsky and Geoffrey Hinton, of AlexNet, a convolutional neural network</li>
          <li>Co-author of the AlphaGo paper</li>
          <li>Postdoc with Andrew Ng at Stanford University</li>
          <li>Joined Hinton's new research company DNNResearch, acquired by Google</li>
          <li>Worked on Tensorflow</li>
          <li>450860 citations</li>
          <li><a href="https://en.wikipedia.org/wiki/Ilya_Sutskever" target="_blank">Wikipedia</a></li>
        </ul>

      </section>

  
      <!-- Social Impact Section -->
      <section>
        <h2>Social Impact</h2>
        <p>As the zero-shot task transfer ability of large-scale language models discovered in this paper became the backbone for GPT-3.5 and GPT-4, the social impact of this paper can be tied to the social impact of these later models. Many have raised concerns about GPT and other models' potential to replace human jobs and thus leave many people unemployed. As these models evolve, there is potential to use these models to generate misinformation as in the case of fake news and malicious content raising ethical considerations. </p>
        <p>A potential positive social impact is by offloading repetitive uncreative work to an AI; it leaves humans more free to spend time and mental energy on creative and innovative work. The free nature of GPT 3 also provides a writing, coding, and useful tool in many other areas to anyone who has access to a computer, which has a potential equalizing factor in opportunity.</p>

      </section>
  
      <!-- Industry Applications Section -->
      <section>
        <h2>Industry Applications</h2>
        <ul>
          <li><strong>Text Generation:</strong></li>
           <ul>
             <li><strong>Content Creation for Marketing and Advertising:</strong> GPT-2 could be used to automatically generate marketing copy, advertising content, or product descriptions. It could tailor the language and tone based on specified guidelines, allowing businesses to automate the creation of engaging and persuasive content.</li>
             <li><strong>E-learning and Education:</strong> GPT-2 could assist in creating diverse and contextually appropriate quiz questions for educational platforms, aiding in the development of interactive learning materials.</li>
             <li><strong>Gaming and Virtual Environments:</strong> GPT-2 can contribute to generating dialogue, narratives, and immersive storylines in video games, virtual reality environments, and other digital interactive experiences.</li>
           </ul>
          <li><strong>Chatbot Development:</strong></li>
           <p><strong>Customer Support and Service:</strong> GPT-2 could power a chatbot that provides detailed and contextually relevant responses to customer queries, improving the quality of automated technical support.</p>
          <li><strong>Text Summarization</strong></li>
           <p><strong>Healthcare:</strong> GPT-2 could be used to generate concise and understandable summaries of medical documents or research papers, making healthcare information more accessible.</p>
        </ul>
      </section>
  
      <!-- Follow-up Research Section -->
      <section>
        <h2>Follow-up Research</h2>

        <!-- Why is it important from an academic point of view? -->
        <h3>Why is it important from an academic point of view?</h3>
        <p>This paper is important academically because it demonstrates the ability of a large-scale language model to learn not only on the tasks it was trained on but other tasks that it was not been trained on, i.e., zero-shot task transfer. This was done without any explicit supervision and accounted for memorization in the training data set. This was the first time that such a large scale of transferable learning between various tasks without supervision was discovered, and it was done without changing the model from task to task but simply by utilizing a large data set. The discovery of this property would become the critical foundation for GPT-3.5 and other large-scale language models.</p>

        <!-- Future academic research directions -->
        <h3>Future academic research directions</h3>
        <p>The authors realize the limitations of GPT-2 in terms of practicality, and that it did a poor job on summarization. The authors also mainly tested GPT-2's performance on NLP tasks but acknowledge that there are many other tasks on which GPT-2 could be evaluated on. The authors also highlight the potential of improvement of performance of GPT-2 with further fine-tuning. As we now know with GPT-3.5 and GPT-4 further research was eventually conducted by changing the model and training it on an even larger data set. Further research can also be conducted on the various parts of the architecture and what contributes to this zero-shot task transfer, as this paper mainly evaluated the performance of the model and did not go into detail about what specifically about the large-scale language model allowed it to complete these tasks.</p>

      </section>

  
      <!-- Peer Review Section -->
      <section>
        <h2>Peer Review</h2>

        <!-- Summary -->
        <h4>Summary</h4>
        <p>The paper presents GPT-2, a large-scale unsupervised language model capable of performing multiple natural language processing tasks without task-specific training. GPT-2 is trained on a diverse range of internet text and uses a transformer architecture to generate coherent and contextually relevant text. Unlike traditional models that are trained for specific tasks, GPT-2 demonstrates the ability to generate high-quality text across various tasks, including language translation, summarization, and question-answering. The model's unsupervised learning approach allows it to generalize well to different domains and tasks, showcasing its versatility as a multitask learner. The paper discusses the model's strengths, such as its ability to capture long-range dependencies in text, as well as its limitations and future research directions.</p>

        <!-- Strengths and Weaknesses -->
        <h4>Strengths and Weaknesses</h4>
        <ul>
          <li><strong>Originality:</strong> While GPT-2 is based on the architecture of GPT-1, the discovery that large scale language models can do zero-shot task transfer unsupervised was an original and novel discovery.</li>
          <li><strong>Quality:</strong> The submission is technically sound and demonstrates the model's ability through several NLP tasks.</li>
          <li><strong>Clarity:</strong> The paper is clearly written.</li>
          <li><strong>Significance:</strong> The discovery of this new ability of large language models is highly significant and eventually became the foundation for GPT-3.5 and GPT-4.</li>
        </ul>

        <!-- Limitations -->
        <h4>Limitations</h4>
        <p>Authors address their limitations (see Further Research section of the blog).</p>

        <!-- Review Ratings -->
        <h4>Review Ratings</h4>
        <ul>
          <li><strong>Soundness:</strong> 4 (Excellent)</li>
          <li><strong>Presentation:</strong> 4 (Excellent)</li>
          <li><strong>Contribution:</strong> 4 (Excellent)</li>
          <li><strong>Overall:</strong> 8 (Strong Accept)</li>
          <li><strong>Confidence:</strong> 3 (You are fairly confident in your assessment.)</li>
        </ul>

      </section>


      <!-- Diagrams Section -->
      <section>
        <h2>Diagrams</h2>

        <!-- Diagram 1 -->
        <div class="diagram">
          <h4>Diagram 1</h4>
          <p>We observe a consistent improvement in zero-shot performance across various NLP tasks as the number of parameters in the GPT-2 model increases. This suggests that enlarging the size and capacity of the language model enhances its ability to learn relevant features during pre-training. While the GPT model may not reach the performance levels of some specific tasks achieved by other models, its impressive performance is noteworthy, especially considering its understanding of text without task-specific fine-tuning.</p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 1.png" alt="Diagram 1">
        </div>

        <!-- Diagram 2 -->
        <div class="diagram">
          <h4>Diagram 2</h4>
          <p>The fig shows GPT-2 model performance steadily increases, achieving new state-of-the-art results on common nouns (93.3%) and named entities (89.1%) in the CBT. The Children’s Book Test (CBT) assesses language model performance across various word categories, including named entities, nouns, verbs, and prepositions to predict the correct option among 10 choices for an omitted word. </p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 2.png" alt="Diagram 2">
        </div>

        <!-- Diagram 3 -->
        <div class="diagram">
          <h4>Diagram 3</h4>
          <p>The Winograd Schema challenge evaluates systems for commonsense reasoning by assessing their ability to resolve ambiguities in text.GPT-2 achieves a state-of-the-art accuracy of 70.70%, marking a 7% improvement over previous results.</p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 3.png" alt="Diagram 3">
          
        </div>

        <!-- Diagram 4 -->
        <div class="diagram">
          <h4>Diagram 4</h4>
          <p>The performance of GPT-2 model on both the training and test sets of WebText are similar and improve together as model size is increased suggesting that the model hasnt completely learned the patterns form the dataset and there is still potential to train bigger models and for longer.</p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 4.png" alt="Diagram 4">
          
        </div>

      </section>
  
    </div><!--col-->
  </div><!--row -->
  

<h3>References</h3>

<p><a name="Radford 2019">[1]</a> <a href="https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf"
  >Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever.
  <em>Language Models are Unsupervised Multitask Learners</em></a>
</p>

<h2>Team Members</h2>
                                                   
<p>Fatima Tourk and Shruti Biradar</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
