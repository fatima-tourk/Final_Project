<!doctype html>
<html lang="en">
<head>
<title>Language Models are Unsupervised Multitask Learners</title>
<meta property="og:title" content="Language Models are Unsupervised Multitask Learners" />
<meta name="twitter:title" content="Language Models are Unsupervised Multitask Learners" />
<meta name="description" content="Examining a paper demonstrating the ability of large scale language models to complete zero shot task transfer." />
<meta property="og:description" content="Examining a paper demonstrating the ability of large scale language models to complete zero shot task transfer." />
<meta name="twitter:description" content="Examining a paper demonstrating the ability of large scale language models to complete zero shot task transfer." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Language Models are Unsupervised Multitask Learners</nobr>
 <nobr class="widenobr">for CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of Language Models are Unsupervised Multitask Learners</h2>
<p>Examining a paper demonstrating the ability of large scale language models (GPT-2) to complete zero shot task transfer, a foundational property of subsequent GPT-3.5 and GPT-4 models.</p>
</div>
</div>
<div class="row">
<div class="col">

  <div class="row">
    <div class="col">
  
      <!-- Literature Review Section -->
      <section>
        <h2>Literature Review</h2>
        <p>Your literature review content goes here.</p>
      </section>
  
      <!-- Biography Section -->
      <section>
        <h2>Biography</h2>

        <!-- Alec Radford -->
        <h3>Alec Radford</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/alec radford.jpeg" alt="Alec Radford Image" width="300" height="200">
          <li>The original paper on generative pre-training of a transformer-based language model was written by Alec Radford (GPT-1)</li>
          <li>Franklin W. Olin College of Engineering</li>
          <li>Head of research at indico data solutions</li>
          <li>ML developer/researcher at OpenAI</li>
          <li>104767 citations</li>
          <li><a href="https://twitter.com/AlecRad" target="_blank">Twitter</a></li>
          <li><a href="https://www.linkedin.com/in/alecradford/" target="_blank">LinkedIn</a></li>
        </ul>

        <!-- Jeffrey Wu -->
        <h3>Jeffrey Wu</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/jeff wu.jpg" alt="Jeffrey Wu Image" width="300" height="200">
          <li>MIT Undergraduate</li>
          <li>MIT Masters in Probabilistic Programming Languages</li>
          <li>2nd employee at startup Terminal.com, left after 4 years and Terminal was later acquired by Udacity</li>
          <li>2 years at Google working on deep learning for user personalization</li>
          <li>Now at OpenAI on the alignment team, working to ensure that AI is truthful and helpful</li>
          <li>40650 citations</li>
          <li><a href="https://www.wuthejeff.com/about/" target="_blank">Website</a></li>
        </ul>

        <!-- Rewon Child -->
        <h3>Rewon Child</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/rewon child.png" alt="Rewon Child Image" width="200" height="200">
          <li>2022 - Present: Founding team, Member of Technical Staff at Inflection</li>
          <li>2021 - 2022: Research at Google Brain/Search on large models.</li>
          <li>2018 - 2020: Algorithms team at OpenAI</li>
          <li>2016 - 2017: Speech team at Andrew Ng's lab at Baidu Research.</li>
          <li>2015 - 2016: Enlitic, a startup focused on applying deep learning to medical imaging.</li>
          <li>44589 citations</li>
          <li><a href="https://rewonc.github.io//" target="_blank">Website</a></li>
        </ul>

        <!-- David Luan -->
        <h3>David Luan</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/david-figs-v1B.png" alt="David Luan Image">
          <li>VP of engineering at OpenAI</li>
          <li>Most of my career has revolved around near- and long-term impacts of AI on society</li>
          <li>Certificate in CS from Worcester State from when I was 12, and a BS/BA in Applied Math and Political Science from Yale</li>
          <li>Former director of AI at Axon</li>
          <li>Founded and was CEO of Dextro</li>
          <li>11276 citations</li>
          <li><a href="http://www.davidluan.com/" target="_blank">Website</a></li>
        </ul>

        <!-- Dario Amodei -->
        <h3>Dario Amodei</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/dario amodei.jpeg" alt="Dario Amodei Image">
          <li>CEO and Co-founder of anthropic, AI safety and research company</li>
          <li>Quit OpenAI to build rival company in 2020</li>
          <li>Was VP of research, research director, and team lead for AI safety</li>
          <li>Senior Research scientist at google</li>
          <li>Research Scientist at Baidu Inc with Andrew Ng</li>
          <li>Undergraduate from Stanford in Physics</li>
          <li>PhD from Princeton in Biophysics</li>
          <li>44885 citations</li>
          <li><a href="https://www.linkedin.com/in/dario-amodei-3934934/" target="_blank">LinkedIn</a></li>
        </ul>

        <!-- Ilya Sutskever -->
        <h3>Ilya Sutskever</h3>
        <ul>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/ilya sutskever.jpg" alt="Ilya Sutskever Image" width="300" height="200">
          <li>Co-founder and chief scientist at OpenAI</li>
          <li>Co-inventor, with Alex Krizhevsky and Geoffrey Hinton, of AlexNet, a convolutional neural network</li>
          <li>Co-author of the AlphaGo paper</li>
          <li>Postdoc with Andrew Ng at Stanford University</li>
          <li>Joined Hinton's new research company DNNResearch, acquired by Google</li>
          <li>Worked on Tensorflow</li>
          <li>450860 citations</li>
          <li><a href="https://en.wikipedia.org/wiki/Ilya_Sutskever" target="_blank">Wikipedia</a></li>
        </ul>

      </section>

  
      <!-- Social Impact Section -->
      <section>
        <h2>Social Impact</h2>
        <p>As the zero-shot task transfer ability of large-scale language models discovered in this paper became the backbone for GPT 3.5 and GPT 4, the social impact of this paper can be tied to the social impact of these later models. Many have raised concerns about GPT and other modelsâ€™ potential to replace human jobs and thus leave many people unemployed. A potential positive social impact is by offloading repetitive uncreative work to an AI; it leaves humans more free to spend time and mental energy on creative and innovative work. The free nature of GPT 3 also provides a writing, coding, and useful tool in many other areas to anyone who has access to a computer, which has a potential equalizing factor in opportunity.</p>

      </section>
  
      <!-- Industry Applications Section -->
      <section>
        <h2>Industry Applications</h2>
        <p>Your industry applications content goes here.</p>
      </section>
  
      <!-- Follow-up Research Section -->
      <section>
        <h2>Follow-up Research</h2>

        <!-- Why is it important from an academic point of view? -->
        <h3>Why is it important from an academic point of view?</h3>
        <p>This paper is important academically because it demonstrates the ability of a large-scale language model to learn not only on the tasks it was trained on but other tasks that it was not been trained on, i.e., zero-shot task transfer. This was done without any explicit supervision and accounted for memorization in the training data set. This was the first time that such a large scale of transferable learning between various tasks without supervision was discovered, and it was done without changing the model from task to task but simply by utilizing a large data set. The discovery of this property would become the critical foundation for GPT-3.5 and other large-scale language models.</p>

        <!-- Future academic research directions -->
        <h3>Future academic research directions</h3>
        <p>The authors realize the limitations of GPT-2 in terms of practicality, and that it did a poor job on summarization. The authors also mainly tested GPT-2's performance on NLP tasks but acknowledge that there are many other tasks on which GPT-2 could be evaluated on. The authors also highlight the potential of improvement of performance of GPT-2 with further fine-tuning. As we now know with GPT-3.5 and GPT-4 further research was eventually conducted by changing the model and training it on an even larger data set. Further research can also be conducted on the various parts of the architecture and what contributes to this zero-shot task transfer, as this paper mainly evaluated the performance of the model and did not go into detail about what specifically about the large-scale language model allowed it to complete these tasks.</p>

      </section>

  
      <!-- Peer-Review Section -->
      <section>
        <h2>Peer-Review</h2>
        <p>Your peer-review content goes here.</p>
      </section>

      <!-- Diagrams Section -->
      <section>
        <h2>Diagrams</h2>

        <!-- Diagram 1 -->
        <div class="diagram">
          <p>Diagram 1</p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 1.png" alt="Diagram 1">
          
        </div>

        <!-- Diagram 2 -->
        <div class="diagram">
          <p>Diagram 2</p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 2.png" alt="Diagram 2">
          
        </div>

        <!-- Diagram 3 -->
        <div class="diagram">
          <p>Diagram 3</p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 3.png" alt="Diagram 3">
          
        </div>

        <!-- Diagram 4 -->
        <div class="diagram">
          <p>Diagram 4</p>
          <img src="https://raw.githubusercontent.com/fatima-tourk/Final_Project/master/images/Fig 4.png" alt="Diagram 4">
          
        </div>

      </section>
  
    </div><!--col-->
  </div><!--row -->
  

<h3>References</h3>

<p><a name="Radford 2019">[1]</a> <a href="https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf"
  >Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever.
  <em>Language Models are Unsupervised Multitask Learners</em></a>
</p>

<h2>Team Members</h2>
                                                   
<p>Fatima Tourk and Shruti Biradar</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
